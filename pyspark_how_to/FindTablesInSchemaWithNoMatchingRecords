from pyspark.sql.functions import col

schema_name = 'tbl_schema'
tbl_pattern = 'table_name_pattern_'

# Create a list of table names in the schema
table_names = [table.name for table in spark.catalog.listTables(schema_name)]

# Create an empty list to store the names of tables with no records
empty_tables = []

# Iterate over the tables and check if there are any records that meet the specified criteria
for table_name in table_names:
  if table_name.startswith(tbl_pattern):
    print(table_name)
    df = spark.table(f"{schema_name}.{table_name}")
    count = df.filter(col("mach_id").rlike("^[0-9]+$") & (col("year") == 2022) & (col("month") == 12) & (col("day") == 7)).count()
    if count == 0:
      empty_tables.append(table_name)

# Print the names of tables with no records
print(empty_tables)
